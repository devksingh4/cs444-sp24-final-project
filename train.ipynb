{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "# from models import SegFormer_CS444\n",
    "import transformers\n",
    "from dataset import ContrailsDataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsingh/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b3 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE=256\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = transformers.SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b3\", num_labels=2, image_size=IMAGE_SIZE).to(device) #<- head not pretrtained, we finetune head\n",
    "# overwrite segformer head with our own modifications to use some new tricks\n",
    "from models import SegformerDecodeHeadModified\n",
    "model.decode_head = SegformerDecodeHeadModified(model.config).to(device)\n",
    "model.train()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "unfreeze_layers = ['segformer.encoder.patch_embeddings', 'segformer.encoder.block.2', 'segformer.encoder.block.3', 'segformer.encoder.layer_norm', 'decode_head']  \n",
    "for name, param in model.named_parameters():\n",
    "    for layer_name in unfreeze_layers:\n",
    "        if layer_name in name:\n",
    "            param.requires_grad = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch.optim as optim\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "train_dataset = ContrailsDataset(\"/data/contrails/train\")\n",
    "val_dataset =  ContrailsDataset(\"/data/contrails/validation\")\n",
    "class LitSegDeg(L.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(train_dataset, num_workers=4, persistent_workers=True, batch_size=self.batch_size, prefetch_factor=8)\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(val_dataset, num_workers=4, persistent_workers=True, batch_size=self.batch_size, prefetch_factor=8)\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        z = self.model(pixel_values=x, labels=y)\n",
    "        loss, logits = z.loss, z.logits\n",
    "        labels = y\n",
    "        self.log(\"val_loss\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "                # note that the metric expects predictions + labels as numpy arrays\n",
    "                metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "            metrics = metric._compute(\n",
    "                    predictions=predicted.cpu(),\n",
    "                    references=labels.cpu(),\n",
    "                    num_labels=2,\n",
    "                    ignore_index=255,\n",
    "                    reduce_labels=False, # we've already reduced the labels ourselves\n",
    "            )\n",
    "            self.log(\"val_mean_iou\", metrics[\"mean_iou\"])\n",
    "        return z.loss\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        z = self.model(pixel_values=x, labels=y)\n",
    "        loss, logits = z.loss, z.logits\n",
    "        labels = y\n",
    "        self.log(\"train_loss\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "                # note that the metric expects predictions + labels as numpy arrays\n",
    "                metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "            metrics = metric._compute(\n",
    "                    predictions=predicted.cpu(),\n",
    "                    references=labels.cpu(),\n",
    "                    num_labels=2,\n",
    "                    ignore_index=255,\n",
    "                    reduce_labels=False, # we've already reduced the labels ourselves\n",
    "            )\n",
    "            self.log(\"train_mean_iou\", metrics[\"mean_iou\"])\n",
    "        return z.loss\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/dsingh/source/devksingh4/transfer-vit-unet/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                             | Params\n",
      "-----------------------------------------------------------\n",
      "0 | model | SegformerForSemanticSegmentation | 50.4 M\n",
      "-----------------------------------------------------------\n",
      "47.6 M    Trainable params\n",
      "2.8 M     Non-trainable params\n",
      "50.4 M    Total params\n",
      "201.516   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e0ec8d4c55434d9892032ca4630299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsingh/miniconda3/lib/python3.11/site-packages/datasets/features/image.py:348: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f739bd6a684dd194ba7c690270002d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=2, monitor=\"val_loss\")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    min_delta=0.00, \n",
    "    patience=3, \n",
    "    verbose=False, \n",
    "    mode=\"min\",\n",
    ")\n",
    "    \n",
    "l_model = LitSegDeg(model, batch_size=16)\n",
    "trainer = L.Trainer(max_epochs=20, log_every_n_steps=5, callbacks=[checkpoint_callback, early_stop_callback], val_check_interval=400)\n",
    "trainer.fit(model=l_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
